{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Connectionist Bench (Sonar, Mines vs. Rocks) Data Set Arm Identefication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "# fix random seed for reproducibility\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pandas.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "\n",
    "To download the sataset from the web follow this is the link http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\", header=None)\n",
    "print('The dataset has been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical features as numbers\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "def number_encode_features(df):\n",
    "    result = df.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        if result.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "\n",
    "# Calculate the correlation and plot it\n",
    "encoded_data, _ = number_encode_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoded_data.iloc[:,-1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.drop(60 , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# for training data\n",
    "X = encoded_data.astype(float)\n",
    "Input_toNN = X.shape[1]\n",
    "features = preprocessing.scale(X)\n",
    "target = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Extracting 20% validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.40, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training examples',len(X_train))\n",
    "print('Number of validation examples',len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# normalize the data attributes\n",
    "X_train = preprocessing.normalize(X_train)\n",
    "#X_test = preprocessing.normalize(X_test)\n",
    "# standardize the data attributes\n",
    "X_train = preprocessing.scale(X_train)\n",
    "#X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train_Connectionist\", X_train)\n",
    "np.save(\"X_test_Connectionist\", X_test)\n",
    "np.save(\"y_train_Connectionist\", y_train)\n",
    "np.save(\"y_test_Connectionist\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "#The layers where we want to prune\n",
    "N1 = 20 # 20\n",
    "N2 = 20\n",
    "labelsTrain = np_utils.to_categorical(y_train)\n",
    "model = Sequential()\n",
    "model.add(Dense(N1,\n",
    "                input_shape=(Input_toNN,), \n",
    "                activation=\"relu\"))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(N2, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')\n",
    "start_time = time.time()\n",
    "model.fit(X_train, labelsTrain, verbose=0, batch_size=1, epochs=100)\n",
    "print(\"The time for training NN is  %s seconds \" % (time.time() - start_time))\n",
    "loss, accuracy = model.evaluate(X_train, labelsTrain, batch_size=1, verbose=0)\n",
    "accuracy_training_Model = accuracy\n",
    "print(\"Test fraction correct (NN-loss) = {:.2f}\".format(loss))\n",
    "print(\"Test fraction correct (NN-Accuracy) = {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"accuracy_testing_Model =\",accuracy_testing_Model)\n",
    "model.save('my_model.h5') \n",
    "import scipy.io\n",
    "weights = model.get_weights()\n",
    "scipy.io.savemat('parameters.mat', dict(w=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "modelBuckup = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelBuckup.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the layer should have N1 and N2 weights\n",
    "L = 2\n",
    "# Set Threshold and constant\n",
    "Threshold = 0\n",
    "constant=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of rounding same for all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Round = 1800  # will be the same for all methods 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random policy\n",
    "\n",
    "\n",
    "Random policy  or Epsilon-first strategy : during T rounds, sample a random arm (uniform sampling), and then choose the best arms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startP = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "noExamples = len(X_train)\n",
    "NL = np.zeros(Round)\n",
    "RP = np.zeros(Round)\n",
    "Avg_Accumaltive_RP = np.zeros(N1*N2)\n",
    "p_reshape = Avg_Accumaltive_RP.reshape(N1,N2)\n",
    "Count_RP = np.ones(N1*N2)\n",
    "import random\n",
    "count = 0\n",
    "\n",
    "# uniform sampling \n",
    "for j in range(Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            ind = random.randint(0,N1*N2-1)\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            #print(i,k)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            Count_RP[ind]=Count_RP[ind]+1\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            #print(delta)\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            #print(reward)\n",
    "            val = Count_RP[ind]\n",
    "            Avg_Accumaltive_RP[ind] = (val-1)/val * Avg_Accumaltive_RP[ind] + 1/val * reward\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            RP[j]=ind\n",
    "\n",
    "# Exploting \n",
    "# The  weight that has been chosen by Random policy method\n",
    "\n",
    "reshapeRP = Avg_Accumaltive_RP.reshape(N1,N2)\n",
    "P = np.argmax(Avg_Accumaltive_RP)\n",
    "endP = time.time()\n",
    "print(\"Execution time = \",endP - startP)\n",
    "Pi,Pj = np.unravel_index(P, reshapeRP.shape)\n",
    "print(\"\\n The index of chosen  W_ji is = \", Pj, Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_RP\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Random_Policy_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Random_Policy_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Random_Policy_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Random_Policy_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Random_Policy_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Random_Policy_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Random_Policy_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Random_Policy_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On testing dataset (Unseen dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Random_Policy_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Random_Policy_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Random_Policy_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Random_Policy_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startG = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "#noExamples = 100\n",
    "NL = np.zeros(Round)\n",
    "ep = np.zeros(Round)\n",
    "Avg_Accumaltive_R_EGN = np.zeros(N1*N2)\n",
    "p_reshape = Avg_Accumaltive_R_EGN.reshape(N1,N2)\n",
    "Count_EGN = np.ones(N1*N2)\n",
    "import random\n",
    "epsilon=0.5\n",
    "count = 0\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_R_EGN[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            if (epsilon>random.uniform(0, 1)):\n",
    "                ind = np.argmax(Avg_Accumaltive_R_EGN)\n",
    "            else:\n",
    "                ind = random.randint(0,N1*N2-1)\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            #print(i,k)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            Count_EGN[ind]=Count_EGN[ind]+1\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            #print(delta)\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            #print(reward)\n",
    "            val = Count_EGN[ind]\n",
    "            Avg_Accumaltive_R_EGN[ind] = (val-1)/val * Avg_Accumaltive_R_EGN[ind] + 1/val * reward\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            ep[j]=ind\n",
    "endG = time.time()\n",
    "print(\"Execution time \",endG - startG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Epsilon Greedy method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeG = Avg_Accumaltive_R_EGN.reshape(N1,N2)\n",
    "G = np.argmax(Avg_Accumaltive_R_EGN)\n",
    "Gi,Gj = np.unravel_index(G, reshapeG.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Gj, Gi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_R_EGN\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Epsilon_Greedy_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Epsilon_Greedy_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Epsilon_Greedy_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Epsilon_Greedy_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Epsilon_Greedy_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Epsilon_Greedy_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Epsilon_Greedy_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Epsilon_Greedy_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On testing dataset (Unseen dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Epsilon_Greedy_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Epsilon_Greedy_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Epsilon_Greedy_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Epsilon_Greedy_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startU = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(Round)\n",
    "Avg_Accumaltive_UCB = np.zeros(N1*N2)\n",
    "Count_UCB = np.ones(N1*N2)\n",
    "UCB1 = np.zeros(Round)\n",
    "p_reshape = Avg_Accumaltive_UCB.reshape(N1,N2)\n",
    "count = 0\n",
    "import random\n",
    "tau=4\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_UCB[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            padding = np.sqrt(2*Count_UCB.sum()/Count_UCB)\n",
    "            ucb = Avg_Accumaltive_UCB + padding\n",
    "            ind = np.argmax(ucb)\n",
    "            Count_UCB[ind] = Count_UCB[ind] + 1\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0           \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            val = Count_UCB[ind]\n",
    "            Avg_Accumaltive_UCB[ind] = (val-1)/val * Avg_Accumaltive_UCB[ind] + 1/val * reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            UCB1[j]=ind\n",
    "endU = time.time()\n",
    "print(\"Execution time \",endU - startU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by UCB1 method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeU = Avg_Accumaltive_UCB.reshape(N1,N2)\n",
    "U = np.argmax(Avg_Accumaltive_UCB)\n",
    "Ui,Uj = np.unravel_index(U, reshapeU.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Uj, Ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_UCB\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('UCB1_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('UCB1_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('UCB1_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('UCB1_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('UCB1_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('UCB1_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('UCB1_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('UCB1_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('UCB1_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('UCB1_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('UCB1_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('UCB1_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startT = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(Round)\n",
    "Avg_Accumaltive_TS = np.zeros(N1*N2)\n",
    "Count_TS = np.ones(N1*N2)\n",
    "TS = np.zeros(Round)\n",
    "p_reshape = Avg_Accumaltive_TS.reshape(N1,N2)\n",
    "count = 0\n",
    "success = np.zeros(N1*N2)\n",
    "failure = np.zeros(N1*N2)\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            if(delta>0):\n",
    "                reward = 1\n",
    "                success[i] = success[i]+1\n",
    "            else:\n",
    "                reward = 0\n",
    "                failure[i] = failure[i]+1                        \n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_TS[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer \n",
    "            ind = np.argmax(np.random.beta(1+success, 1+failure))\n",
    "            Count_TS[ind] = Count_TS[ind] + 1\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0                     \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            if(delta>0):\n",
    "                reward = 1\n",
    "                success[i] = success[i]+1\n",
    "            else:\n",
    "                reward = 0\n",
    "                failure[i] = failure[i]+1            \n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)          \n",
    "            val = Count_TS[ind]\n",
    "            Avg_Accumaltive_TS[ind] = (val-1)/val * Avg_Accumaltive_TS[ind] + 1/val * reward\n",
    "            TS[j]=ind\n",
    "endT = time.time()\n",
    "print(\"Execution time \",endT - startT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Thompson Sampling method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeT = Avg_Accumaltive_TS.reshape(N1,N2)\n",
    "T = np.argmax(Avg_Accumaltive_TS)\n",
    "Ti,Tj = np.unravel_index(T, reshapeT.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Tj, Ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_TS\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Thompson_Sampling_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Thompson_Sampling_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Thompson_Sampling_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Thompson_Sampling_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Thompson_Sampling_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Thompson_Sampling_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Thompson_Sampling_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Thompson_Sampling_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Thompson_Sampling_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Thompson_Sampling_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Thompson_Sampling_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Thompson_Sampling_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successive Rejects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a K-armed bandit, Successive Rejects operates in (K − 1) phases. At the end of each phase, the arm with the lowest average reward is discarded. Thus, at the end of phase (K − 1) only one arm survives, and this arm is recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startS = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(noExamples)\n",
    "sr = []\n",
    "Avg_Accumaltive_SR = np.zeros(N1*N2)\n",
    "Avg_Accumaltive_SR2 = np.zeros(N1*N2)\n",
    "Avg_Accumaltive_SR1 = np.zeros(N1*N2)\n",
    "p_reshape = Avg_Accumaltive_SR2.reshape(N1,N2)\n",
    "check_array = np.ones((N1,N2))\n",
    "Count_SR = np.ones(N1*N2)\n",
    "A = [0]\n",
    "Nk = []\n",
    "K = N1*N2\n",
    "Log = 0.5\n",
    "for k in range(K):\n",
    "    d = k+2\n",
    "    Log = Log + 1/d\n",
    "for k in range(K-2):\n",
    "    d = k+1\n",
    "    nK = int(np.floor(1/Log * (Round-K)/(K+1-d)))\n",
    "    if nK!=0:\n",
    "        A.append(nK)\n",
    "A.sort(reverse=False)\n",
    "#print(\"The round of the phases : \",A)\n",
    "g=0\n",
    "for a in A:\n",
    "    h = a - g\n",
    "    g = a\n",
    "    Nk.append(h)\n",
    "    count=0\n",
    "    #print(a)\n",
    "    for n in range(h):\n",
    "        c=0\n",
    "        for i in range(N1):\n",
    "            for j in range(N2):\n",
    "                if check_array[i][j]==1:\n",
    "                    b = random.randint(0,noExamples-1) \n",
    "                    loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "                    All_weights=modelBuckup.get_weights()\n",
    "                    temp = All_weights[2][i][j]\n",
    "                    All_weights[2][i][j] = 0\n",
    "                    modelBuckup.set_weights(All_weights)\n",
    "                    loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "                    delta = loss_New - loss\n",
    "                    reward = max(0,Threshold + delta)/constant\n",
    "                    All_weights[2][i][j] = temp\n",
    "                    modelBuckup.set_weights(All_weights)\n",
    "                    val = Count_SR[c]\n",
    "                    #print(reward)\n",
    "                    Avg_Accumaltive_SR[c] = (val-1)/val * Avg_Accumaltive_SR[c] + 1/val * reward\n",
    "                    All_weights[2][i][j] = temp\n",
    "                    modelBuckup.set_weights(All_weights)\n",
    "                    count = count+1\n",
    "                    c = c + 1    \n",
    "        Avg_Accumaltive_SR2=Avg_Accumaltive_SR2+Avg_Accumaltive_SR\n",
    "        Avg_Accumaltive_SR1=Avg_Accumaltive_SR2.copy()\n",
    "    ind = np.argmin(Avg_Accumaltive_SR2)\n",
    "    Avg_Accumaltive_SR2[ind] = 100\n",
    "    #print(Avg_Accumaltive_SR)\n",
    "    s,t = np.unravel_index(ind, p_reshape.shape)\n",
    "    ###check_array[s][t]=0\n",
    "    sr.append(ind)\n",
    "endS = time.time()\n",
    "print(\"Execution time \",endS - startS)\n",
    "#print(\"A = \", A)\n",
    "#print(\"Nk = \", Nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_Accumaltive_SR3 = np.abs(100-Avg_Accumaltive_SR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Successive Rejects method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeS = Avg_Accumaltive_SR3.reshape(N1,N2)\n",
    "S = np.argmax(Avg_Accumaltive_SR3)\n",
    "Si,Sj = np.unravel_index(S, reshapeS.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Sj, Si)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_SR3\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Successive_Rejects_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Successive_Rejects_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Successive_Rejects_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Successive_Rejects_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Successive_Rejects_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Successive_Rejects_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Successive_Rejects_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Successive_Rejects_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Successive_Rejects_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Successive_Rejects_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Successive_Rejects_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Successive_Rejects_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence\n",
    "\n",
    "\n",
    "\n",
    "The implimetation based on:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K. Terayama, H. Iwata, M. Araki, Y. Okuno, K. Tsuda, \"Machine Learning Accelerates MD-based Binding-Pose Prediction between Ligands and Proteins\", Bioinformatics, 2017.\n",
    "\n",
    "\n",
    "Gabillon, V.; Ghavamzadeh, M.; Lazaric, A. \"Best arm identification: A unified approach to fixed budget and fixed confidence.\" NIPS, pp.3212–3220, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.25\n",
    "def beta(s, budget, K, a, mean_list, automatic = False, auto_para = 1, H = 1):\n",
    "    if automatic:\n",
    "        alpha = auto_para\n",
    "\n",
    "        a = alpha* (budget - K)/(4*H)\n",
    "        return np.sqrt(a / s)\n",
    "    else:\n",
    "        return np.sqrt(a*(budget - K) / s)\n",
    "\n",
    "def U(i, t, budget, pulls, a):\n",
    "    K = len(pulls)\n",
    "    return np.mean(pulls[i]) + beta(len(pulls[i]), budget, K, a)\n",
    "\n",
    "def L(i, t, budget, pulls, a):\n",
    "    K = len(pulls)\n",
    "    return np.mean(pulls[i]) - beta(len(pulls[i]), budget, K, a)\n",
    "\n",
    "def B(i, t, budget, pulls, a, K):\n",
    "    list_woi = range(K)\n",
    "    list_woi.pop(i)\n",
    "    return np.max([U(j, t, budget, pulls, a) - L(i, t, budget, pulls, a)  for j in list_woi])\n",
    "\n",
    "def calc_B(k, U_l, L_l, K, max_U_i_t_index, max_U_i_t, max_U_i_eq_k):\n",
    "    if k == max_U_i_t_index:\n",
    "        return max_U_i_eq_k - L_l[k]\n",
    "    else:\n",
    "        return max_U_i_t - L_l[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "modelBuckup = load_model('my_model.h5')\n",
    "startUB = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(Round)\n",
    "Avg_Accumaltive_UB = np.zeros(N1*N2)\n",
    "Count_UB = np.ones(N1*N2)\n",
    "UB = np.zeros(Round)\n",
    "p_reshape = Avg_Accumaltive_UB.reshape(N1,N2)\n",
    "count = 0\n",
    "import random\n",
    "K = N1*N2\n",
    "# Play each arm once\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_UB[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "#Exploration loop\n",
    "for j in range(N1*N2-1, Round):\n",
    "            mean_list = [np.mean(Avg_Accumaltive_UB[i]) for i in range(K)]\n",
    "            beta_list = [beta(len([Avg_Accumaltive_UB[i]]), Round, K, a, mean_list) for i in range(K)]\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            U_list = [mean_list[i] + beta_list[i] for i in range(K)]\n",
    "            L_list = [mean_list[i] - beta_list[i] for i in range(K)]\n",
    "            max_U_i_t_index = np.argmax(U_list)\n",
    "            max_U_i_t = U_list[max_U_i_t_index]\n",
    "            max_U_i_eq_k = np.max(cp.copy(U_list).pop(max_U_i_t_index))\n",
    "            B_list = [calc_B(k, U_list, L_list, K, max_U_i_t_index, max_U_i_t, max_U_i_eq_k) for k in range(K)]\n",
    "            J_t = np.argmin([B_list[i] if Count_UB[i] < 20 else 10**10 for i in range(K)])\n",
    "            list_woJt = list(range(K))\n",
    "            list_woJt.pop(J_t)\n",
    "            u_t = list_woJt[np.argmax([U_list[i] if Count_UB[i] < 20 else -10**10 for i in list_woJt])]\n",
    "            l_t = J_t\n",
    "            I_t = [l_t, u_t][np.argmax([beta_list[i] for i in [l_t, u_t]])]   \n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            Count_UB[I_t] = Count_UB[I_t] + 1     \n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0           \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            val = Count_UB[I_t]\n",
    "            Avg_Accumaltive_UB[I_t] = (val-1)/val * Avg_Accumaltive_UB[I_t] + 1/val * reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            UB[j]=I_t            \n",
    "endUB = time.time()\n",
    "print(\"Execution time \",endUB - startUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_Accumaltive_UB3 = np.abs(100-Avg_Accumaltive_UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Successive Rejects method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeS = Avg_Accumaltive_UB3.reshape(N1,N2)\n",
    "S = np.argmax(Avg_Accumaltive_UB3)\n",
    "Si,Sj = np.unravel_index(S, reshapeS.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Sj, Si)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_UB3\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Unified_Approach_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Unified_Approach_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Unified_Approach_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Unified_Approach_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Unified_Approach_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Unified_Approach_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Unified_Approach_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Unified_Approach_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Unified_Approach_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Unified_Approach_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Unified_Approach_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Unified_Approach_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startSM = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "#noExamples = 100\n",
    "NL = np.zeros(Round)\n",
    "SM = np.zeros(Round)\n",
    "Avg_Accumaltive_R_SM= np.zeros(N1*N2)\n",
    "p_reshape = Avg_Accumaltive_R_SM.reshape(N1,N2)\n",
    "Count_SM = np.ones(N1*N2)\n",
    "import random\n",
    "tau=4\n",
    "count = 0\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_R_SM[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "\n",
    "            \n",
    "            prob = np.exp(Avg_Accumaltive_R_SM/tau)\n",
    "            sum = prob.sum()\n",
    "            prb = prob/sum\n",
    "            ind = np.random.choice(numpy.arange(0, N1*N2), p=prb.reshape(N1*N2))\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            \n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            Count_SM[ind]=Count_SM[ind]+1\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            #print(delta)\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            #print(reward)\n",
    "            val = Count_SM[ind]\n",
    "            Avg_Accumaltive_R_SM[ind] = (val-1)/val * Avg_Accumaltive_R_SM[ind] + 1/val * reward\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            SM[j]=ind\n",
    "endSM = time.time()\n",
    "print(\"Execution time \",endSM - startSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Softmax method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeSM = Avg_Accumaltive_R_SM.reshape(N1,N2)\n",
    "G = np.argmax(Avg_Accumaltive_R_SM)\n",
    "Gi,Gj = np.unravel_index(G, reshapeSM.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Gj, Gi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_R_SM\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Softmax_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Softmax_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Softmax_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('Softmax_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('Softmax_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('Softmax_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('Softmax_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('Softmax_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On testing dataset (Unseen dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('Softmax_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('Softmax_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('Softmax_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('Softmax_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win–Stay, Lose–Shift (Pursuit method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startWSLS = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "#noExamples = 100\n",
    "NL = np.zeros(Round)\n",
    "WSLS = np.zeros(Round)\n",
    "Avg_Accumaltive_R_WSLS= np.zeros(N1*N2)\n",
    "\n",
    "Probs = 1/(N1*N2) * np.ones(N1*N2)\n",
    "\n",
    "p_reshape = Avg_Accumaltive_R_WSLS.reshape(N1,N2)\n",
    "Count_WSLS = np.ones(N1*N2)\n",
    "import random\n",
    "beta = 0.3\n",
    "count = 0\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            if reward>0:\n",
    "                Probs[count] = Probs[count] + beta * (1-Probs[count])\n",
    "            else:\n",
    "                Probs[count] = Probs[count] - beta * Probs[count]\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_R_WSLS[count] = reward\n",
    "            \n",
    "            \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            \n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)      \n",
    "            \n",
    "            \n",
    "            ind = np.argmax(Probs)\n",
    "            Count_WSLS[ind] = Count_WSLS[ind] + 1\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            \n",
    "            \n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            #print(delta)\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            if reward>0:\n",
    "                Probs[ind] = Probs[ind] + beta * (1-Probs[ind])\n",
    "            else:\n",
    "                Probs[ind] = Probs[ind] - beta * Probs[ind] \n",
    "            count = count+1\n",
    "            #print(reward)\n",
    "            val = Count_WSLS[ind]\n",
    "            Avg_Accumaltive_R_WSLS[ind] = (val-1)/val * Avg_Accumaltive_R_WSLS[ind] + 1/val * reward\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            WSLS[j]=ind\n",
    "endWSLS = time.time()\n",
    "print(\"Execution time \",endWSLS - startWSLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Epsilon Greedy method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeWSLS = Avg_Accumaltive_R_WSLS.reshape(N1,N2)\n",
    "G = np.argmax(Avg_Accumaltive_R_WSLS)\n",
    "Gi,Gj = np.unravel_index(G, reshapeWSLS.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Gj, Gi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_R_WSLS\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('WSLS_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('WSLS_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('WSLS_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('WSLS_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('WSLS_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('WSLS_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('WSLS_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('WSLS_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On testing dataset (Unseen dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('WSLS_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('WSLS_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('WSLS_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('WSLS_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klFunction(x, y, eps=1e-15):\n",
    "    x = max(x, eps)\n",
    "    y = max(y, eps)\n",
    "    return x * np.log(x / y) + (1 - x) * np.log((1 - x) / (1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBuckup = load_model('my_model.h5')\n",
    "startklU = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(Round)\n",
    "Avg_Accumaltive_klUCB = np.zeros(N1*N2)\n",
    "currentq = Avg_Accumaltive_klUCB + 0.000001\n",
    "Count_klUCB = np.ones(N1*N2)\n",
    "klUCB = np.zeros(Round)\n",
    "p_reshape = Avg_Accumaltive_klUCB.reshape(N1,N2)\n",
    "count = 0\n",
    "import random\n",
    "tau=4\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            if reward>0:\n",
    "                currentq[count] = (currentq[count] + reward)/ reward\n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_klUCB[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            \n",
    "            count = count+1\n",
    "            d=0\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            ind = np.argmax(currentq)\n",
    "            Count_klUCB[ind] = Count_klUCB[ind] + 1\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            try:\n",
    "                normalized = (Avg_Accumaltive_klUCB - np.min(Avg_Accumaltive_klUCB)) / (np.max(Avg_Accumaltive_klUCB) - np.min(Avg_Accumaltive_klUCB))\n",
    "            except:\n",
    "                normalized = Avg_Accumaltive_klUCB  / np.max(Avg_Accumaltive_klUCB)\n",
    "            q = normalized[ind]\n",
    "            while (q < 1) and (d < np.log(j)/Count_klUCB[ind]):\n",
    "                d = klFunction(normalized[ind], q)  # d(mu_i, q)\n",
    "                q = q + 0.01\n",
    "                currentq[ind]= q\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0           \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            reward = max(0,Threshold + delta)/constant\n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            val = Count_klUCB[ind]\n",
    "            Avg_Accumaltive_klUCB[ind] = (val-1)/val * Avg_Accumaltive_klUCB[ind] + 1/val * reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            UCB1[j]=ind\n",
    "endklU = time.time()\n",
    "print(\"Execution time \",endklU - startklU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by KL-UCB method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapeklU = Avg_Accumaltive_klUCB.reshape(N1,N2)\n",
    "klU = np.argmax(Avg_Accumaltive_klUCB)\n",
    "Ui,Uj = np.unravel_index(klU, reshapeU.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Uj, Ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_klUCB\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('kl_UCB_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('kl_UCB_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('kl_UCB_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('kl_UCB_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('kl_UCB_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('kl_UCB_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('kl_UCB_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('kl_UCB_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('kl_UCB_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('kl_UCB_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('kl_UCB_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('kl_UCB_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.legend(loc = 3)\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "modelBuckup = load_model('my_model.h5')\n",
    "startbayucb = time.time()\n",
    "N_arry = np.zeros(N1*N2)\n",
    "NL = np.zeros(Round)\n",
    "Avg_Accumaltive_bayucb = np.zeros(N1*N2)\n",
    "Count_bayucb = np.ones(N1*N2)\n",
    "bayucb = np.zeros(Round)\n",
    "p_reshape = Avg_Accumaltive_bayucb.reshape(N1,N2)\n",
    "count = 0\n",
    "success = np.zeros(N1*N2)\n",
    "failure = np.zeros(N1*N2)\n",
    "for i in range(N1): \n",
    "    for j in range(N2):\n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "        # Prune the neuron in the layer\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][j]\n",
    "            All_weights[2][i][j] = 0 \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[1:10], labelsTrain[1:10], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            if(delta>0):\n",
    "                reward = 1\n",
    "                success[i] = success[i]+1\n",
    "            else:\n",
    "                reward = 0\n",
    "                failure[i] = failure[i]+1                        \n",
    "            All_weights[2][i][j]= temp\n",
    "            Avg_Accumaltive_bayucb[count] = reward\n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            count = count+1\n",
    "for j in range(N1*N2-1, Round):\n",
    "            b = random.randint(0,noExamples-1)  \n",
    "            loss, accuracy = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            \n",
    "            ind = np.argmax(beta.pdf(1-1/Count_bayucb, 1+success, 1+failure))\n",
    "        \n",
    "            Count_bayucb[ind] = Count_bayucb[ind] + 1\n",
    "            i,k = np.unravel_index(ind, p_reshape.shape)\n",
    "            All_weights=modelBuckup.get_weights()\n",
    "            temp = All_weights[2][i][k]\n",
    "            All_weights[2][i][k] = 0                     \n",
    "            modelBuckup.set_weights(All_weights)\n",
    "            loss_New, accuracy_New = modelBuckup.evaluate(X_train[b:b+1], labelsTrain[b:b+1], batch_size=1, verbose=0)\n",
    "            delta = loss_New - loss\n",
    "            if(delta>0):\n",
    "                reward = 1\n",
    "                success[i] = success[i]+1\n",
    "            else:\n",
    "                reward = 0\n",
    "                failure[i] = failure[i]+1            \n",
    "            All_weights[2][i][k] = temp\n",
    "            modelBuckup.set_weights(All_weights)          \n",
    "            val = Count_bayucb[ind]\n",
    "            Avg_Accumaltive_bayucb[ind] = (val-1)/val * Avg_Accumaltive_bayucb[ind] + 1/val * reward\n",
    "            TS[j]=ind\n",
    "endbayucb = time.time()\n",
    "print(\"Execution time \",endbayucb - startbayucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The single weight that has been chosen by Bayes UCB method\n",
    "\n",
    "Practically we choose k weights that have high rewards but the purpose of this file to show the computation time and which is the method be able to choose best arm comparing to direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshapebayucb = Avg_Accumaltive_bayucb.reshape(N1,N2)\n",
    "bayucb = np.argmax(Avg_Accumaltive_bayucb)\n",
    "Ti,Tj = np.unravel_index(bayucb, reshapebayucb.shape)\n",
    "print(\"The index of chosen  W_ji is = \", Tj, Ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune 5%, 10%, 25% and 50% from the unpruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finsh playing start pruining:')\n",
    "rewards = Avg_Accumaltive_bayucb\n",
    "Pecent = {1:'5%', 2:'10%', 3:'25%', 4:'50%'}\n",
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "No_ofweights = len(weights_unpruned)\n",
    "accuracyL = []\n",
    "\n",
    "# Prune 5%\n",
    "No_of_pruned_weight = int(ListPecent[0] * N1*N2)\n",
    "for t in range(No_of_pruned_weight):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 5% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('bayucb_model_5_Pecernt.h5') \n",
    "\n",
    "# Prune 10%\n",
    "No_of_pruned_weight_10 = int(ListPecent[1] * N1*N2)\n",
    "for t in range(No_of_pruned_weight, No_of_pruned_weight_10):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 10% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('bayucb_model_10_Pecernt.h5') \n",
    "\n",
    "# Prune 25%\n",
    "No_of_pruned_weight_25 = int(ListPecent[2] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_10, No_of_pruned_weight_25):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 25% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('bayucb_model_25_Pecernt.h5')\n",
    "\n",
    "\n",
    "# Prune 50%\n",
    "No_of_pruned_weight_50 = int(ListPecent[3] * N1*N2)\n",
    "for t in range(No_of_pruned_weight_25, No_of_pruned_weight_50):\n",
    "    x = np.argmax(rewards)\n",
    "    i,k = np.unravel_index(x, p_reshape.shape)\n",
    "    rewards[x] = -100\n",
    "    All_weights = modelBuckup.get_weights()\n",
    "    All_weights[2][i][k] = 0\n",
    "    modelBuckup.set_weights(All_weights)\n",
    "loss, accuracy = modelBuckup.evaluate(X_train, labelsTrain, batch_size=1, verbose=2)\n",
    "print(\"Accuract after prune 50% is \", accuracy)\n",
    "accuracyL.append(accuracy)\n",
    "modelBuckup.save('bayucb_model_50_Pecernt.h5') \n",
    "x= [5,10,25,50]\n",
    "xx = [accuracy_training_Model, accuracy_training_Model, accuracy_training_Model, accuracy_training_Model]\n",
    "plt.plot(x, accuracyL, '+')\n",
    "plt.plot(x, accuracyL, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the weights before pruning\n",
    "weights_unpruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_unpruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_unpruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - Unpruned Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 5% pruning\n",
    "model = load_model('bayucb_model_5_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 5% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 10% pruning\n",
    "model = load_model('bayucb_model_10_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 10% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 25% pruning\n",
    "model = load_model('bayucb_model_25_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 25% of the Model')\n",
    "plt.show()\n",
    "\n",
    "# distribution of the weights 50% pruning\n",
    "model = load_model('bayucb_model_50_Pecernt.h5')\n",
    "weights = model.get_weights()\n",
    "weights_pruned = []\n",
    "for wei in weights:\n",
    "    wei = wei[wei!=0]\n",
    "    weights_pruned.extend(wei)\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(weights_pruned, 100, range = (-0.5,0.5), alpha=0.6)\n",
    "plt.xlabel('Magnitude of weights')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.title('Distribution of the magnitude of Weights - pruned 50% of the Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "\n",
    "model = load_model('bayucb_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5 on testing data = \", accuracy_testing_Model_05)\n",
    "\n",
    "model = load_model('bayucb_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_10 = accuracy\n",
    "print(\"The accuracy of the model after pruned 10 on testing data = \", accuracy_testing_Model_10)\n",
    "\n",
    "model = load_model('bayucb_model_25_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_25 = accuracy\n",
    "print(\"The accuracy of the model after pruned 25 on testing data = \", accuracy_testing_Model_25)\n",
    "\n",
    "model = load_model('bayucb_model_50_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_50 = accuracy\n",
    "print(\"The accuracy of the model after pruned 50 on testing data = \", accuracy_testing_Model_50)\n",
    "\n",
    "accuracyL_test = [accuracy_testing_Model_05, accuracy_testing_Model_10, accuracy_testing_Model_25\n",
    "                  , accuracy_testing_Model_50]\n",
    "xx = [accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model, accuracy_testing_Model]\n",
    "plt.plot(x, accuracyL_test, '+')\n",
    "plt.plot(x, accuracyL_test, 'b--.', label='after pruning')\n",
    "plt.plot(x, xx, 'k--.', label='before pruning')\n",
    "plt.ylabel('Accuracy after pruning')\n",
    "plt.title('Accuracy vs Sparsity')\n",
    "plt.xlabel('Percentage of Sparsity')\n",
    "plt.legend(loc = 3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListPecent = [0.05, 0.1, 0.25, 0.5]\n",
    "# 5% of the model\n",
    "modelBuckup = load_model('my_model.h5')\n",
    "fivePercent  = int(ListPecent[0] * N1*N2)\n",
    "All_weights = modelBuckup.get_weights()\n",
    "p_reshape = All_weights[2].reshape(N1*N2)\n",
    "idx = np.argpartition(np.abs(p_reshape), fivePercent)\n",
    "p_reshape[idx[:fivePercent]] = 0\n",
    "modelBuckup.set_weights(All_weights)\n",
    "modelBuckup.save('sparse_model_5_Pecernt.h5') \n",
    "\n",
    "# 10% of the model\n",
    "modelBuckup = load_model('my_model.h5')\n",
    "tenPercent  = int(ListPecent[1] * N1*N2)\n",
    "All_weights = modelBuckup.get_weights()\n",
    "p_reshape = All_weights[2].reshape(N1*N2)\n",
    "idx = np.argpartition(np.abs(p_reshape), tenPercent)\n",
    "p_reshape[idx[:tenPercent]] = 0\n",
    "modelBuckup.set_weights(All_weights)\n",
    "modelBuckup.save('sparse_model_10_Pecernt.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Accuracy = []\n",
    "model1 = load_model('sparse_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model1.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model after pruning 5% on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Accuracy = []\n",
    "model = load_model('sparse_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model after pruning 10% on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution time for different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution time of Epsilon Greedy Method     =\",endG - startG)\n",
    "print(\"Execution time of UCB1 Method               =\",endU - startU)\n",
    "print(\"Execution time of Thompson Sampling Method  =\",endT - startT)\n",
    "print(\"Execution time of Successive Rejects Method =\",endS - startS)\n",
    "print(\"Execution time of Unified Approach Method   =\",endUB - startUB)\n",
    "print(\"Execution time of Random policy Method      =\",endP - startP)\n",
    "print(\"Execution time of Softmax Method            =\",endSM - startSM)\n",
    "print(\"Execution time of Win–Stay, Lose–Shift      =\",endWSLS - startWSLS)\n",
    "print(\"Execution time of KL-UCB Method             =\",endklU - startklU)\n",
    "print(\"Execution time of Bayes-UCB Method          =\",endbayucb - startbayucb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = {'Name of Method': [\"Epsilon Greedy\", \"UCB1\", \"Thompson Sampling\", \"Successive Rejects\",\"Unified Approach\",\"Random policy\", \"Softmax\", \"Win–Stay, Lose–Shift\",\"KL-UCB\",\"bayes UCB\"], \n",
    "        'Execution Time': [endG - startG, endU - startU, endT - startT, endS - startS, endUB - startUB, endP - startP, endSM - startSM, endWSLS - startWSLS, endklU - startklU, endbayucb - startbayucb]}\n",
    "df_time = pd.DataFrame.from_dict(Time)\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "df_time.plot.bar(x='Name of Method', y='Execution Time', rot=0, color ='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuarcy before pruning and after pruned 5% using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Accuracy = []\n",
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "#Random Policy\n",
    "model = load_model('Random_Policy_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Random Policy on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Epsilon Greedy\n",
    "model = load_model('Epsilon_Greedy_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Epsilon Greedy on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#UCB1\n",
    "model = load_model('UCB1_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using UCB1 on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Thompson Sampling\n",
    "model = load_model('Thompson_Sampling_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Thompson Sampling on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Unified Approach\n",
    "model = load_model('Unified_Approach_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Unified Approach on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Successive Rejects\n",
    "model = load_model('Successive_Rejects_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Successive Rejects on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Softmax\n",
    "model = load_model('Softmax_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Softmax on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Win–Stay, Lose–Shift\n",
    "model = load_model('WSLS_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Win–Stay, Lose–Shift on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#KL-UCB\n",
    "model = load_model('kl_UCB_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using KL-UCB on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Bayes-UCB\n",
    "model = load_model('bayucb_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Bayes-UCB on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "# Sparsity\n",
    "model1 = load_model('sparse_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model1.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruning 5% using Sparsity method on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "Methods = [\"Model\", \"Random policy\", \"Epsilon Greedy\", \"UCB1\", \"Thompson Sampling\", \"Unified Approach\",\"Successive Rejects\",\"Softmax\" ,\"Win–Stay, Lose–Shift\", \"KL-UCB\",\"Bayes-UCB\", \"Sparsity\"]\n",
    "\n",
    "acc = {'Name of Method': Methods, \n",
    "        'Acceracy': Testing_Accuracy}\n",
    "df_accauracy = pd.DataFrame.from_dict(acc)\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "df_accauracy.plot.bar(x='Name of Method', y='Acceracy', rot=0, color ='gold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuarcy before pruning and after pruned 10% using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Accuracy = []\n",
    "model = load_model('my_model.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"The accuracy of the model before pruning on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "#Random Policy\n",
    "model = load_model('Random_Policy_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Random Policy on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Epsilon Greedy\n",
    "model = load_model('Epsilon_Greedy_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Epsilon Greedy on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#UCB1\n",
    "model = load_model('UCB1_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using UCB1 on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Thompson Sampling\n",
    "model = load_model('Thompson_Sampling_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Thompson Sampling on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Unified Approach\n",
    "model = load_model('Unified_Approach_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Unified Approach on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Successive Rejects\n",
    "model = load_model('Successive_Rejects_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Successive Rejects on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Softmax\n",
    "model = load_model('Softmax_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Softmax on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Win–Stay, Lose–Shift\n",
    "model = load_model('WSLS_model_5_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 5% using Win–Stay, Lose–Shift on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#KL-UCB\n",
    "model = load_model('kl_UCB_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using KL-UCB on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "#Bayes-UCB\n",
    "model = load_model('bayucb_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model_05 = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruned 10% using Bayes-UCB on testing data = \", accuracy_testing_Model_05)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "# Sparsity\n",
    "model1 = load_model('sparse_model_10_Pecernt.h5')\n",
    "labelsTest = np_utils.to_categorical(y_test)\n",
    "loss, accuracy = model1.evaluate(X_test, labelsTest, batch_size=1, verbose=0)\n",
    "accuracy_testing_Model = accuracy\n",
    "print(\"\\nThe accuracy of the model after pruning 10% using Sparsity method on testing data = \", accuracy_testing_Model)\n",
    "Testing_Accuracy.append(accuracy)\n",
    "\n",
    "Methods = [\"Model\", \"Random policy\", \"Epsilon Greedy\", \"UCB1\", \"Thompson Sampling\", \"Unified Approach\",\"Successive Rejects\",\"Softmax\" ,\"Win–Stay, Lose–Shift\", \"KL-UCB\",\"Bayes-UCB\", \"Sparsity\"]\n",
    "\n",
    "\n",
    "acc = {'Name of Method': Methods, \n",
    "        'Acceracy': Testing_Accuracy}\n",
    "df_accauracy = pd.DataFrame.from_dict(acc)\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "df_accauracy.plot.bar(x='Name of Method', y='Acceracy', rot=0, color ='gold');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
